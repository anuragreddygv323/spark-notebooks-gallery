{
  "metadata" : {
    "name" : "cryptocurrency_ts",
    "user_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ "com.twosigma % flint_2.11 % 0.2.0-SNAPSHOT" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "B66668E92375476781C605315E3AB45D"
    },
    "cell_type" : "markdown",
    "source" : "# Analyzing time series cryptocurrencies data"
  }, {
    "metadata" : {
      "id" : "C7BACF654F9C41A48863E880E57FBBC2"
    },
    "cell_type" : "markdown",
    "source" : "This notebook is highly inspired by [@sarahpan](https://blog.timescale.com/@sarahpan) great [article](https://blog.timescale.com/analyzing-ethereum-bitcoin-and-1200-cryptocurrencies-using-postgresql-3958b3662e51) on analysis of cryptocurrencies data using PostgreSQL.\nThe main goal of this notebook is not to provide you with Ethereum, Bitcoin and other cryptocurrencies insights (for this one please refer to @sarahpan's article) but to get you familar with tools which Apache Spark ecosystem can provide you to perform similar kind of analysis."
  }, {
    "metadata" : {
      "id" : "4706A09A8E15495AAF864E9A7A21D304"
    },
    "cell_type" : "markdown",
    "source" : "In this notebook we will perform time series data analysis using **Apache Spark**, a time series library for Apache Spark called **[Flint](https://github.com/twosigma/flint)** and interactive computaions and visualization capabilities of **Spark Notebook**."
  }, {
    "metadata" : {
      "id" : "D7C3BF23967E40299ACAD5979C030C81"
    },
    "cell_type" : "markdown",
    "source" : "## Data"
  }, {
    "metadata" : {
      "id" : "5BB9D4082ACF4DA0AC325E7D6555DEC1"
    },
    "cell_type" : "markdown",
    "source" : "[Direct link](https://timescaledata.blob.core.windows.net/datasets/crypto_data.tar.gz) to download the dataset. \nAlso [here](https://blog.timescale.com/analyzing-ethereum-bitcoin-and-1200-cryptocurrencies-using-postgresql-downloading-the-dataset-a1bbc2d4d992)\none can find detailed description for the dataset.\n\nWe are interested in two files:\n - btc_prices.csv — A CSV file with the daily BTC price data (OHLCV format), spanning seven years from 2010 to 2017 across multiple currencies (e.g. USD, CAD, EUR, CNY, etc.)\n - crypto_prices.csv — A CSV file with the daily cryptocurrency price data (OHLCV format) for over 1200 cryptocurrencies since 2013\n\n\nThis is a small dataset and Spark lanched on laptop in local mode should be enough to work with it."
  }, {
    "metadata" : {
      "id" : "9E98D8BA912F476183047905BA6A02BA"
    },
    "cell_type" : "markdown",
    "source" : "## Requirements"
  }, {
    "metadata" : {
      "id" : "EBE3F5BD66104E1FB405CB0B1A884B99"
    },
    "cell_type" : "markdown",
    "source" : "`spark 2.0` or higher, `2.11.7 or higher`. \nWe also need to provide custom dependencies on [Flint](https://github.com/twosigma/flint) library. \nIt's not published on maven central repository but you can simpliy build it from the source and publish locally by running\n```\nsbt publishLocal\n```\n\nAfter that provide coordinates in `customDeps` section of Notebook metadata (Edit -> Edit Notebook Metadata):\n\n```\n\"customDeps\": [\n    \"com.twosigma % flint_2.11 % 0.2.0-SNAPSHOT\"\n  ]\n```"
  }, {
    "metadata" : {
      "id" : "27C1E1EF127F4774899A4353DCE79CC9"
    },
    "cell_type" : "markdown",
    "source" : "## Reading the data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "7B86726A63C442BE821572870875F9F0"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "C56940B83424472E8351CF99DC00A87C"
    },
    "cell_type" : "code",
    "source" : "val spark = sparkSession",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "9F1336C097294206B0B0EDCCEBA73E80"
    },
    "cell_type" : "markdown",
    "source" : "Apache Spark supports direct read from `csv` files and can try to automatically infer schema.\nBut we also can provide our own schema while reading `csv` data. \nThis will prevent from full scan of `csv` file to infer schema and it's also more accurate if we know what we're doing."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "728B6EC9ABE84A2980BD3F02FD231405"
    },
    "cell_type" : "code",
    "source" : "val btcPriceSchema = StructType(\n    StructField(\"datetime\", StringType, false) ::\n    StructField(\"opening_price\", DoubleType, false) ::\n    StructField(\"highest_price\", DoubleType, false) ::\n    StructField(\"lowest_price\", DoubleType, false) ::\n    StructField(\"closing_price\", DoubleType, false) ::\n    StructField(\"volume_btc\", DoubleType, false) ::\n    StructField(\"volume_currency\", DoubleType, false) ::\n    StructField(\"currency_code\", StringType, false) :: Nil\n)\n\nval cryptoPriceSchema = StructType(\n    StructField(\"datetime\", StringType, false) ::\n    StructField(\"opening_price\", DoubleType, false) ::\n    StructField(\"highest_price\", DoubleType, false) ::\n    StructField(\"lowest_price\", DoubleType, false) ::\n    StructField(\"closing_price\", DoubleType, false) ::\n    StructField(\"volume_crypto\", DoubleType, false) ::\n    StructField(\"volume_btc\", DoubleType, false) ::\n    StructField(\"currency_code\", StringType, false) :: Nil\n)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "6AE15A64CF6D41FB8777CDBEA6483C79"
    },
    "cell_type" : "code",
    "source" : "val btcPricesDF = spark.read\n  .format(\"csv\")\n  .schema(btcPriceSchema)\n  .load(\"/path/to/crypto_data/btc_prices.csv\")\n  .withColumn(\"time\", unix_timestamp($\"datetime\", \"yyyy-MM-dd HH:mm:ssX\"))\n  .withColumn(\"date\", from_unixtime($\"time\", \"yyyy-MM-dd\"))\n\nval cryptoPricesDF = spark.read\n  .format(\"csv\")\n  .schema(cryptoPriceSchema)\n  .load(\"/path/to/crypto_data/crypto_prices.csv\")\n  .withColumn(\"time\", unix_timestamp($\"datetime\", \"MM/dd/yyyy HH:mm\"))\n  .withColumn(\"date\", from_unixtime($\"time\", \"yyyy-MM-dd\"))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "BD15BCA0C36A43EFBB6D5AA3B2F0D6E0"
    },
    "cell_type" : "code",
    "source" : "btcPricesDF.show(5)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "9D9039DA7D57498C9E5EB15958EA5E45"
    },
    "cell_type" : "code",
    "source" : "val btcUSDPricesDF = btcPricesDF.where($\"currency_code\" === \"USD\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "63C55BF359444F208E01C54907EFA654"
    },
    "cell_type" : "code",
    "source" : "cryptoPricesDF.show(5)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "38E4B7CD053B4C75B5A1DE0CB125CB18"
    },
    "cell_type" : "markdown",
    "source" : "## OHLC charts"
  }, {
    "metadata" : {
      "id" : "792474D0482A49568DEF0B7A45817840"
    },
    "cell_type" : "markdown",
    "source" : "Since the data is presented in OHLCV format the natural way to plot them would be using OHLC charts.\nSpark Notebook comes with build in support (`CustomPlotlyChart`) for [Plotly javascript API](https://plot.ly/javascript/) for data visualizations.\nTo get some examples on usage of `CustomPlotlyChart` refer to notebooks from `notebooks/viz` directory which comes with Spark Notebook distribution or explore online with [nbviewer](https://viewer.kensu.io/notebooks/viz/00_Data%20Visualisation%20With%20Plotly.snb)."
  }, {
    "metadata" : {
      "id" : "6F1A2DCB39014C168AF92ED707C728FA"
    },
    "cell_type" : "markdown",
    "source" : "Resulting Plotly charts are interactive so fill free to zoom and hover over data directly from the Notebook."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "ED6B95585618448A8B97335689306F77"
    },
    "cell_type" : "code",
    "source" : "CustomPlotlyChart(btcUSDPricesDF,\n                  layout=\"{title: 'BTC price in USD', showlegend: false}\",\n                  dataOptions=\"{type: 'ohlc'}\",\n                  dataSources=\"\"\"{\n                    x: 'date',\n                    close: 'closing_price',\n                    high: 'highest_price',\n                    low: 'lowest_price',\n                    open: 'opening_price'\n                  }\"\"\",\n                 maxPoints=3000)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "BCF7F7603A6B43F98EBC4DB63FB76461"
    },
    "cell_type" : "markdown",
    "source" : "\n## TimeSeriesRDD"
  }, {
    "metadata" : {
      "id" : "1DFA903F53754359B8F3491CC5119FD5"
    },
    "cell_type" : "markdown",
    "source" : "Now let's start with some time series analysis. From Flint getting started guide you can find that the entry point into all functionalities for time series analysis in Flint is the `TimeSeriesRDD` class or object. And we can create one from an existing `DataFrame`, for that we have to make sure the `DataFrame` contains a column named \"time\" of type LongType.\nAnd that's why we performed that extra `.withColumn(\"time\", unix_timestamp($\"datetime\", \"yyyy-MM-dd HH:mm:ssX\"))` steps\nto create `btcPricesDF` and `cryptoPricesDF` dataframes."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "4A1B9DF1367E42758755ED783747B711"
    },
    "cell_type" : "code",
    "source" : "import com.twosigma.flint.timeseries.TimeSeriesRDD\nimport scala.concurrent.duration._",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "23801219EBF3454D87E591F3BE45AA29"
    },
    "cell_type" : "code",
    "source" : "val btcUSDPricesTsRdd = TimeSeriesRDD\n  .fromDF(dataFrame = btcUSDPricesDF)(isSorted = true, timeUnit = SECONDS)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "ADAC7B12FBBF4E05B85197C0C0C54E7C"
    },
    "cell_type" : "markdown",
    "source" : "After creating our `TimeSeriesRDD` we can perform various transformations on it. \nIf we want to perform some grouping or aggregation on our time series data then there are several options provided by Flint for that depending on your needs. \nLet's say we want to split our data into `14 day` time buckets and obtain some summary information per each bucket."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "EB2A98F2EB3743AE8D16D57F74832DA5"
    },
    "cell_type" : "markdown",
    "source" : "### Generating clockTS RDD for time buckets"
  }, {
    "metadata" : {
      "id" : "72006D465DD94A828FEC982E64875F2E"
    },
    "cell_type" : "markdown",
    "source" : "Time buckets could be defined by another `TimeSeriesRDD`. Its timestamps will be used to defined intervals, i.e. two sequential timestamps define an interval."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "590F510E372D48FB96F0A5E082D43CA0"
    },
    "cell_type" : "code",
    "source" : "import com.twosigma.flint.timeseries.Summarizers",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "C63F83115F424A13A3BD057C0DB61922"
    },
    "cell_type" : "code",
    "source" : "val timeBin = Duration(14, DAYS).toSeconds.toInt\n\nval minMaxTs = btcUSDPricesDF.select(min($\"time\"), max($\"time\")).head\n\nval (minTs, maxTs) = (minMaxTs(0).asInstanceOf[Long],  minMaxTs(1).asInstanceOf[Long])",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "53D487E7769543CEA8D3388493F51EC8"
    },
    "cell_type" : "code",
    "source" : "val clockTs = minTs to maxTs by timeBin",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "822A53590B61494E8B1BD9640F425675"
    },
    "cell_type" : "code",
    "source" : "val clockTsRdd = TimeSeriesRDD\n  .fromDF(dataFrame = spark.sparkContext.parallelize(clockTs).toDF(\"time\"))(isSorted = true, timeUnit = SECONDS)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "3BF5220A2A18417380327D2C7DD1240D"
    },
    "cell_type" : "markdown",
    "source" : "Now that we defined time intervals we're able to apply available summarizers for each interval."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "5AF9954AF0404E918EB95407C8A35759"
    },
    "cell_type" : "code",
    "source" : "val meanClosingPricesByTwoWeeks = btcUSDPricesTsRdd\n                                    .summarizeIntervals(clockTsRdd, Summarizers.mean(\"closing_price\"))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "D86192F3122544248C34E28693069381"
    },
    "cell_type" : "markdown",
    "source" : "So we obtained `mean` closing price in USD per each `14 day` interval."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "AAF76AF4F301443BAD6781BF4C553DD9"
    },
    "cell_type" : "code",
    "source" : "CustomPlotlyChart(meanClosingPricesByTwoWeeks.toDF.withColumn(\"date\", from_unixtime(($\"time\" / 1e9), \"yyyy-MM-dd\")),\n                  layout=\"\"\"{\n                    title: 'BTC mean closing price in USD over the last seven years (in 14 day intervals)', \n                    showlegend: false, \n                    yaxis: {title: 'Mean Closing Price (BTC/USD)'}}\"\"\",\n                  dataOptions=\"{type: 'scatter'}\",\n                  dataSources=\"\"\"{\n                    x: 'date',\n                    y: 'closing_price_mean'\n                  }\"\"\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "3E53563E0A2B456D8135CA4FC218A41D"
    },
    "cell_type" : "markdown",
    "source" : "##  Spark SQL Window Functions"
  }, {
    "metadata" : {
      "id" : "5DD9C0A855D0401F96D89A4E19CD3CFF"
    },
    "cell_type" : "markdown",
    "source" : "Another useful thing in Apache Spark toolbox is [Spark SQL Window Functions](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html). So what it can be useful for? It  provides the ability to perform calculations across set of rows like calculating a moving average, calculating a cumulative sum, or accessing the values of a row appearing before the current row.\n\nLet's take an example of calculating the day-by-day volatility of BTC\nwhere we want to calculate BTC daily return as a factor of the previous day’s rate."
  }, {
    "metadata" : {
      "id" : "E9E379B977AD490D8E13F051EACDA532"
    },
    "cell_type" : "markdown",
    "source" : "For that we can use `DataFrame` API or use `SQL` expressions by registering temporary view of our dataframe."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "F1A428EA255A4959BE38761CF4C6489C"
    },
    "cell_type" : "code",
    "source" : "btcUSDPricesDF.createOrReplaceTempView(\"btc_usd_prices\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "9E116C20DB47467A87D8A860539B6D97"
    },
    "cell_type" : "code",
    "source" : "val dailyBTCreturnsDF = spark.sql(\"\"\"\nSELECT time,\n       closing_price / lead(closing_price) over prices AS daily_factor\nFROM (\n   SELECT time,\n          closing_price\n   FROM btc_usd_prices\n   GROUP BY 1,2\n) sub window prices AS (ORDER BY time DESC)\n\"\"\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "C4D424E20D7A473086136871C4814E75"
    },
    "cell_type" : "code",
    "source" : "CustomPlotlyChart(dailyBTCreturnsDF.withColumn(\"date\", from_unixtime($\"time\", \"yyyy-MM-dd\")),\n                  layout=\"\"\"{\n                    title: 'BTC daily return (as a factor of the previous day’s rate) over the last seven years', \n                    showlegend: false, \n                    yaxis: {title: 'Daily Return (BTC/USD)', type: 'log'}}\"\"\",\n                  dataOptions=\"{type: 'scatter', line: {width: 1}}\",\n                  dataSources=\"\"\"{\n                    x: 'date',\n                    y: 'daily_factor'\n                  }\"\"\",\n                 maxPoints=3000)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "F70B55C266134A2D8EF0DDE33F8618EC"
    },
    "cell_type" : "markdown",
    "source" : "## Volumes by currency"
  }, {
    "metadata" : {
      "id" : "A4C0A06438DB4DB588632CD412E389CC"
    },
    "cell_type" : "markdown",
    "source" : "Now let's get back to `TimeSeriesRDD` and refresh our knowledge on time seriese data summarization\nwith one more example.\nLet's say we want to track changes in volume of BTC in different fiat currencies in 14 day intervals.\nFor that we can use already seen `.summarizeIntervals` method with additional `key` argument to group results by `currency_code`."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "8C60586CB5484BEA8B9D91467E38FE3C"
    },
    "cell_type" : "code",
    "source" : "val btcPricesTsRdd = TimeSeriesRDD.fromDF(dataFrame = btcPricesDF)(isSorted = false, timeUnit = SECONDS)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "61BDD2D698E141588CE9DD434EA51493"
    },
    "cell_type" : "code",
    "source" : "val btcVolumeByCurrencyByInterval = btcPricesTsRdd.summarizeIntervals(clockTsRdd, \n                                                                      Summarizers.sum(\"volume_btc\"),\n                                                                      key=Seq(\"currency_code\"))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "9F9F7B1FE7D841F6942A486DD6E33EC6"
    },
    "cell_type" : "code",
    "source" : "val btcVolumeByCurrencyDF = btcVolumeByCurrencyByInterval.toDF\n                                 .withColumn(\"date\", from_unixtime(($\"time\" / 1e9), \"yyyy-MM-dd\"))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "80DD564E680C48598881E7820FD1122E"
    },
    "cell_type" : "code",
    "source" : "CustomPlotlyChart(btcVolumeByCurrencyDF,\n                  layout=\"\"\"{\n                    title: 'Volume of BTC in different fiat currencies over the last seven years (in 14 day intervals, stacked)',\n                    barmode: 'stack',\n                    yaxis: {title: 'Volume(BTC/fiat)'}}\"\"\",\n                  dataOptions=\"{type: 'bar', splitBy: 'currency_code'}\",\n                  dataSources=\"\"\"{\n                    x: 'date',\n                    y: 'volume_btc_sum'\n                  }\"\"\",\n                 maxPoints=3000)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "7082B59118664B85AFD6EED71E6B815A"
    },
    "cell_type" : "markdown",
    "source" : "and to make closer look at `CNY` currency"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "5BD56C0BE94F4B46A5564CE3C1A1CC00"
    },
    "cell_type" : "code",
    "source" : "CustomPlotlyChart(btcPricesDF.where($\"currency_code\" === \"CNY\").where(year($\"date\") > 2015),\n                  layout=\"\"\"{\n                    title: 'Volume of BTC in CNY over the last year', \n                    showlegend: false, \n                    yaxis: {title: 'Volume (BTC/CNY)'}}\"\"\",\n                  dataOptions=\"{type: 'scatter'}\",\n                  dataSources=\"\"\"{\n                    x: 'date',\n                    y: 'volume_btc'\n                  }\"\"\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "515B4990BAD548F1A5A9A17EC01486ED"
    },
    "cell_type" : "markdown",
    "source" : "## Temporal Join"
  }, {
    "metadata" : {
      "id" : "EB502257695C4337ACBF350CF3B40356"
    },
    "cell_type" : "markdown",
    "source" : "Now we want to obtain ETH prices in fiat currencies.\nBut in `crypto_prices` table we have only btc prices for all other crypto currencies while prices in fiat currencies we have only for BTC in `btc_prices` table."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "A721A4B660DA4383B0D7C896CB2A6026"
    },
    "cell_type" : "code",
    "source" : "val ethBTCPricesTsRdd = TimeSeriesRDD\n  .fromDF(dataFrame = cryptoPricesDF.where($\"currency_code\" === \"ETH\"))(isSorted = false, timeUnit = SECONDS)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "5F486FA7FD334FF892B75B6C72CE75B0"
    },
    "cell_type" : "code",
    "source" : "ethBTCPricesTsRdd.toDF.show(5)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "F4F8B143F6A34AD688471B0D3E7273F3"
    },
    "cell_type" : "markdown",
    "source" : "And this is where `JOIN` comes in handy.\nBut in case of time series data it would be a [Temporal Join](https://github.com/twosigma/flint#temporal-join-functions). And again Flint provide several options for that.\n\nTemproal join functions define a matching criteria over time.\nIt could be an exact match or it can look past or look future to find closest row from other table with timesatmp located within some `tolerance` inteval.\n\n\nSo given BTC prices in fiat currencies for some timestamp in `btc_prices` table we want to find closest ETH prices in BTC within `1 day` from `crypto_prices` table."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "3568A800DF8847C49D44265D3E479847"
    },
    "cell_type" : "code",
    "source" : "val ethPricesTsRdd = btcPricesTsRdd.futureLeftJoin(ethBTCPricesTsRdd, tolerance = \"1d\", leftAlias=\"btc\")\n                                   .keepRows { row: Row => row.getAs[String](\"currency_code\") != null }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "CED67E0BEEBC45AD8A4A3D7A954DA263"
    },
    "cell_type" : "markdown",
    "source" : "we keep only those records for which matching criteria is met. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "C0E84F1D3E494E5D865D06C27A83C428"
    },
    "cell_type" : "code",
    "source" : "CustomPlotlyChart(ethPricesTsRdd.toDF\n                  .withColumn(\"currency_closing_price\", $\"closing_price\" * $\"btc_closing_price\")\n                  .where($\"btc_currency_code\" isin (\"USD\", \"EUR\", \"GBP\")),\n                  layout=\"\"\"{\n                    title: 'Closing price of ETH in three different fiat currencies over the last three years', \n                    yaxis: {title: 'Closing Price (ETH/fiat)'}}\"\"\",\n                  dataOptions=\"{type: 'scatter', splitBy: 'btc_currency_code'}\",\n                  dataSources=\"\"\"{\n                    x: 'date',\n                    y: 'currency_closing_price'\n                  }\"\"\", maxPoints = 3000)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "BE4A6D4A36374DFA8F2DEA6130030803"
    },
    "cell_type" : "markdown",
    "source" : "Prices in different fiat currencies might be in different scales like for `USD` and `CNY`, \nso plotting them on the same chart with single `yaxis` might be not a good idea. For that one can plot them on the same chart but with [multiple yaxes](https://plot.ly/javascript/multiple-axes/) \nor use [subplots](https://plot.ly/javascript/subplots/)."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "8BFE4332896F4BE7A9D9A50B25543142"
    },
    "cell_type" : "code",
    "source" : "CustomPlotlyChart(ethPricesTsRdd.toDF\n                  .withColumn(\"currency_closing_price\", $\"closing_price\" * $\"btc_closing_price\")\n                  .where($\"btc_currency_code\" isin (\"USD\", \"CNY\")),\n                  layout=\"\"\"{\n                    title: 'Closing price of ETH in two different fiat currencies over the last three years',\n                    xaxis: {domain: [0, 0.45]},\n                    yaxis: {title: 'Closing Price (ETH/USD)'},\n                    xaxis2: {domain: [0.55, 1]},\n                    yaxis2: {title: 'Closing Price (ETH/CNY)', anchor: 'x2'}\n                  }\"\"\",\n                  dataOptions=\"\"\"{\n                    type: 'scatter',\n                    splitBy: 'btc_currency_code',\n                    byTrace: {\n                      'USD': {type: 'scatter'},\n                      'CNY': {\n                        type: 'scatter',\n                        xaxis: 'x2',\n                        yaxis: 'y2'\n                      }\n                    }\n                  }\"\"\",\n                  dataSources=\"\"\"{\n                    x: 'date',\n                    y: 'currency_closing_price'\n                  }\"\"\", maxPoints = 2000)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "4971663027AD43078016FB46C7A516E9"
    },
    "cell_type" : "markdown",
    "source" : "# Total trade volume over the past week."
  }, {
    "metadata" : {
      "id" : "E8B3557E9D574B4EBB9B7EE97560DF22"
    },
    "cell_type" : "markdown",
    "source" : "Another good example on using temporal joins would be calculating of total transaction volume in USD\nfor top all crypto currencies in the dataset over the past week."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "A4C5E79D7931421482E2982C35CA33D2"
    },
    "cell_type" : "code",
    "source" : "val cryptoBTCPricesTsRdd = TimeSeriesRDD\n  .fromDF(dataFrame = cryptoPricesDF.where($\"volume_btc\" > 0))(isSorted = false, timeUnit = SECONDS)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "CB89C0134A304F538EB0E4A66B368DA8"
    },
    "cell_type" : "code",
    "source" : "val btcUSDPricesTsRdd = TimeSeriesRDD.fromDF(dataFrame = btcUSDPricesDF)(isSorted = false, timeUnit = SECONDS)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "B981F4FFDA92428B85ED9CFC0FBE8CBB"
    },
    "cell_type" : "markdown",
    "source" : "An imprortant thing to note here is that when performing a join temproal join function tries to find one closest row from right table.\nBut in our case several rows from `crypto_prices` table corresponding to different currencies share the same timestamp \nand we want to join BTC prices for all of them.\nThe solution is to group all rows sharing exactly the same timestamp in `crypto_prices` table using `.groupByCycle` function."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "01CAC262C53A41869F29E23C2A5C7556"
    },
    "cell_type" : "code",
    "source" : "val cryptoPricesTsRddGrouped = btcUSDPricesTsRdd.leftJoin(cryptoBTCPricesTsRdd.groupByCycle(), tolerance = \"1d\", leftAlias=\"btc\")\n                                             .keepRows { row: Row => row.getAs[Array[Row]](\"rows\")  != null }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "171E9B975D2640598DB6DA75A25C9695"
    },
    "cell_type" : "code",
    "source" : "cryptoPricesTsRddGrouped.toDF.select($\"time\", $\"btc_closing_price\", $\"rows\").show(5)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "139863529ACD49C181F7A222DF8FBEFA"
    },
    "cell_type" : "markdown",
    "source" : "After this we can use `explode` function to create a new row for each element in `rows` array which contains all the rows from `crypto_prices` table\nsharing extacly the same timestamp."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "A2BAD0F7159C4DCC8D23FDAC81996E51"
    },
    "cell_type" : "code",
    "source" : "val cryptoBTCPricesDF = cryptoPricesTsRddGrouped.toDF\n  .withColumn(\"currency_row\", explode($\"rows\"))\n  .drop($\"rows\")\n  .select(($\"time\" / 1e9).cast(LongType).as(\"time\"), $\"btc_currency_code\", $\"btc_volume_currency\", $\"btc_closing_price\", \n          $\"currency_row.closing_price\", $\"currency_row.volume_btc\", $\"currency_row.currency_code\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "63B99551E4164D0884063A02FB84BC44"
    },
    "cell_type" : "code",
    "source" : "cryptoBTCPricesDF.show(5)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "A6BE93EE6A2B41C29DE62F09CD01146B"
    },
    "cell_type" : "code",
    "source" : "btcUSDPricesDF.show(5)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "3388FB93A43E42508838CCCFE45F5024"
    },
    "cell_type" : "code",
    "source" : "cryptoBTCPricesDF.createOrReplaceTempView(\"crypto_btc_usd_prices\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "D0ACE5D8273243BA98556B9D9FACBE33"
    },
    "cell_type" : "markdown",
    "source" : "Now we can perform required aggregation on both `crypto_btc_usd_prices` and `btc_usd_prices` tables and `UNION` the results."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "50A1EC4DED6645699F54A0AC9B94D638"
    },
    "cell_type" : "code",
    "source" : "val weekSeconds = Duration(7, DAYS).toSeconds.toInt",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "4DD39F5AA23747D884139448BC2E4ACC"
    },
    "cell_type" : "code",
    "source" : "val cryptoUSDWeekTransactionVolumeDF = spark.sql(s\"\"\"\n-- crypto currencies by total transaction volume (in usd) over the last month\n\nSELECT 'BTC' as currency_code,\n       sum(b.volume_currency) as total_volume_in_usd\nFROM btc_usd_prices b\nWHERE $maxTs - b.time < $weekSeconds\n\nUNION\n\nSELECT c.currency_code as currency_code,\n       sum(c.volume_btc) * avg(c.btc_closing_price) as total_volume_in_usd\nFROM crypto_btc_usd_prices c\nWHERE $maxTs - c.time < $weekSeconds\nGROUP BY c.currency_code\n\nORDER BY total_volume_in_usd DESC\n\"\"\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "270CAD7D1D644D7180D7313F9413685B"
    },
    "cell_type" : "code",
    "source" : "cryptoUSDWeekTransactionVolumeDF.show(10)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "2AFDC7CC4F3845A8BB6DD1A6794A03BE"
    },
    "cell_type" : "code",
    "source" : "CustomPlotlyChart(cryptoUSDWeekTransactionVolumeDF.limit(10),\n                  layout=\"\"\"{\n                    title: 'Total transaction volume in USD for top 10 currencies over the past week (ranked by volume)',\n                    yaxis: {title: 'Total Volume (currency/USD)'}}\"\"\",\n                  dataOptions=\"{type: 'bar', splitBy: 'currency_code'}\",\n                  dataSources=\"\"\"{\n                    x: 'currency_code',\n                    y: 'total_volume_in_usd'\n                  }\"\"\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "8A820584FA964FD7B9F0458B97B1731F"
    },
    "cell_type" : "markdown",
    "source" : "## Conclusion"
  }, {
    "metadata" : {
      "id" : "C940214D9F4E428BAD981FB7E70CC36D"
    },
    "cell_type" : "markdown",
    "source" : "I hope this notebook has given you some ideas on how can you use these great tools like Apache Spark, Flint library, \nSpark Notebook and Plotly scientific graphing library for time serires data analysis. ALso given that Apache Spark is a fast and general engine for big data processing you can use all these tools on much larger datasets in cluster computing environment.\n\n*And again thanks to [@sarahpan](https://blog.timescale.com/@sarahpan)for [sharing](https://blog.timescale.com/analyzing-ethereum-bitcoin-and-1200-cryptocurrencies-using-postgresql-3958b3662e51) her ideas on analysis of given dataset*."
  } ],
  "nbformat" : 4
}